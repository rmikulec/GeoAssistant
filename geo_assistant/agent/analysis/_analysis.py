from typing import Type, Union, Sequence, Self, Callable, Optional
from enum import Enum
from pydantic import BaseModel, Field, create_model, model_validator
from pydantic.json_schema import SkipJsonSchema
from sqlalchemy import Engine, text

from geo_assistant.config import Configuration
from geo_assistant.logging import get_logger
from geo_assistant.agent.analysis.report import GISReport
from geo_assistant.agent.analysis._steps import DEFAULT_STEP_TYPES, _GISAnalysisStep, _SQLStep, _ReportingStep, _SourceTable, _PlotlyMapLayerStep
from geo_assistant.agent.analysis._exceptions import AnalysisSQLStepFailed


logger = get_logger(__name__)


def make_enum(name: str, *values: Sequence[str]) -> Type[Enum]:
    """
    Dynamically constructs an Enum subclass.
    
    Each member will be named as the upper-cased version of the value,
    and its `.value` will be the original string.

    Args:
        name (str): The name of the Enum Class
        *values (Sequence[str]): A sequence of values that will populate the enum
    
    Returns:
        Type[Enum]: The autogenerated Enum Class populated with *values
    """
    members = { val.upper(): val for val in values }
    return Enum(name, members)



class _GISAnalysis(BaseModel):
    """
    Analysis base class, this is not intended to be directly used. Instead call
        `build_model` with the appropriate `step_types`, `fields` and `tables`
    """

    name: str = Field(description="Snake case name of the analysis")
    steps: list[_GISAnalysisStep]
    #Private variables to not be exposed by pydantic, but used for running the analysis
    final_tables: SkipJsonSchema[list[str]] = Field(default_factory=list)
    tables_created: SkipJsonSchema[list[str]] = Field(default_factory=list)

    @classmethod
    def build_model(
        cls, 
        fields: list[str], 
        tables: list[str],
        step_types: list[Type[_GISAnalysisStep]] = DEFAULT_STEP_TYPES, 
    ) -> Type[Self]:
        """
        Returns a new GISAnalysis subclass where each of the step models
        (AggregateStep, MergeStep) has had:
            - _DynamicField fields replaced with a `Fields` Enum
            - _SourceTable fields replaced with a `Tables` Enum

        This class is intended to be built in order to be used for OpenAI Structured Outputs.
        OpenAI will take in a user query, and attempt to build out an *Analysis* plan, that can
        then be executed by calling `_GISAnalysis.execute`. The class is dynamically built in order
        to ensure complete control over what fields and tables OpenAI can use to generate query
        parameters
        
        Args:
            fields (list[str]): List of fields to limit OpenAI to use
            tables (list[str]): list of tables to limit OpenAI to use
            step_types list[Type[_GISAnalysisStep]]: List of step types that will be used
                for the final schema. Defaults to using all available.
        
        Returns:
            Type[Self]: A new class type, that extends itself as a base class, adding the enum
                restrictions in place of dynamic field descriptors (_DynamicField, _SourceTable)
        """
        fields_enum = make_enum("Fields", *fields)
        tables_enum = make_enum("Tables", *tables)
        # generate dynamic versions of each SQLStep subclass
        dynamic_steps = [
            step_model._build_step_model(fields_enum=fields_enum, tables_enum=tables_enum)
            for step_model in step_types
        ]
        # build Union typing
        StepUnion = Union[tuple(dynamic_steps)]  # type: ignore[misc]

        # override only the 'steps' field
        return create_model(
            cls.__name__.removeprefix('_'),
            __base__=cls,
            steps=(list[StepUnion], ...)
        )
    

    @property
    def output_tables(self) -> list[_SourceTable]:
        """
        List of all the tables that are / will be created when executing analysis
        """
        return [
            step.output_table
            for step in self.steps
            if issubclass(step.__class__, _SQLStep)
        ]
    
    @property
    def sql_steps(self) -> list[_SQLStep]:
        """
        List of all Sql steps
        """
        return [
            step
            for step in self.steps
            if issubclass(step.__class__, _SQLStep)
        ]
    
    @property
    def reporting_steps(self) -> list[_ReportingStep]:
        """
        List of all Reporting steps
        """
        return [
            step
            for step in self.steps
            if issubclass(step.__class__, _ReportingStep)
        ]

    @model_validator(mode="after")
    def _fill_in_source_tables(self):
        """
        Validator updates *any* field in *any* step that is a source table to:
            - If it has an `output_table_idx` value -> Fill in source_table and source_schema
                where source_schema is the name of the analysis
            - If it does not have a `output_table_idx` -> Fill in the source_schema as the
                default defined in `geo_assistant.config`
        """
        # While filling in sources, keep track of any tables that should avoid being immediately
        #   dropped. These are any tables that are used as a source in a `_ReportingStep`
        for step in self.steps:
            for field, info in step.__class__.model_fields.items():
                ann = info.annotation
                if isinstance(ann, type) and issubclass(ann, _SourceTable):
                    value: _SourceTable = getattr(step, field)
                    if value.output_table_idx is not None:
                        new_value = _SourceTable(
                            source_table=self.output_tables[value.output_table_idx],
                            source_schema=self.name,
                            output_table_idx=None
                        )
                        # If the table is also included in a ReportingStep, then it is considered
                        #   'final' and should not be dropped on analysis completion
                        if issubclass(step.__class__, _ReportingStep):
                            self.final_tables.append(str(new_value))
                    else:
                        new_value = _SourceTable(
                            source_table=value.source_table.value,
                            source_schema=Configuration.db_base_schema,
                            output_table_idx=None
                        )
                    setattr(step, field, new_value)
        return self

    async def execute(self, id_: str, engine: Engine, socket_emit: Callable = None, query: str = None) -> GISReport:
        """
        Executes the pregenerated plan. This will populate a new schema in the database, filled
            with any tables that this particular analysis used. It returns a strucutred "GISReport"
            of the results for each step in the analysis. This report can then be interpreted or
            displayed in any way.
        
        Args:
            engine (sqlalchemy.Engine): A sqlalchemy engine to be used to run the query.
        
        Returns:
            GISReport: A pydantic model containing all the results from each step in the analysis
        """

        # Create the new schema (if it doesnt exists already) and grant pg-tileserv permissions to
        #   use it
        with engine.begin() as conn:
            sql = text(
                (
                    f"CREATE SCHEMA IF NOT EXISTS {self.name} AUTHORIZATION {Configuration.db_tileserv_role};"
                    f"GRANT USAGE ON SCHEMA {self.name} TO {Configuration.db_tileserv_role};"
                )
            )
            conn.execute(sql)

        items = []
        # Run each step, saving result to the 'items' array
        for i, step in enumerate(self.steps):
            logger.info(f"Running {step.name}: {step.reasoning}")
            if socket_emit:
                await socket_emit(
                    {
                        "type": "analysis",
                        "query": query,
                        "step": step.reasoning,
                        "id": id_,
                        "status": "running",
                        "progress": float(i+1)/len(self.steps)
                    }
                )

            if isinstance(step, _SQLStep):
                try:
                    items.append(step._execute(engine, self.name))
                    self.tables_created.append(f"{self.name}.{step.output_table}")
                except Exception as e:
                    raise AnalysisSQLStepFailed(
                        analysis_name=self.name,
                        step=step,
                        exception=e
                    )
            # TODO: If any more reporting steps get added, new logic will need to be implemented
            #   here
            elif isinstance(step, _PlotlyMapLayerStep):
                items.append(step.export())
            
        return GISReport(
            items=items
        )